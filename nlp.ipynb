{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "practical No.1"
      ],
      "metadata": {
        "id": "PWeKKI5yuZNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V6S6E0eoadD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('nps_chat')\n",
        "\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "\n",
        "#1. Python program to tokenize the sentence: \"Natural Language Processing with Python is fun!\" into words.\n",
        "import nltk\n",
        "\n",
        "# Sentence to tokenize\n",
        "sentence = \"Natural Language Processing with Python is fun!\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "words\n",
        "\n",
        "# 2. Using NLTK, write a program to find the frequency distribution into words in the next of \"Moby Dick\" by Herman Melville\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "moby_dick_words = gutenberg.words('melville-moby_dick.txt')\n",
        "\n",
        "words_lowercase = [word.lower() for word in moby_dick_words]\n",
        "\n",
        "fdist = FreqDist(words_lowercase)\n",
        "\n",
        "print(\"Most common 10 words: \",fdist.most_common(10))\n",
        "\n",
        "print(\"Full frequency distribution: \",fdist)\n",
        "\n",
        "#3. Create a bigram collocation finder using NLTK for the text of \"Sense and sensibility\" by Jane Austen and list the top 5 bigrams.\n",
        "import nltk\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sense_and_sensibility_words = gutenberg.words('austen-sense.txt')\n",
        "words_lowercase = [word.lower() for word in sense_and_sensibility_words]\n",
        "bigrams = list(nltk.bigrams(words_lowercase))\n",
        "\n",
        "bigram_finder = BigramCollocationFinder.from_words(words_lowercase)\n",
        "top_bigrams = bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n",
        "\n",
        "print(\"Top 5 bigrams:\")\n",
        "for bigram in top_bigrams:\n",
        "    print(bigram)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P593DxkFW1f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-eO8pNLOW2nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjxfwCODW2yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZ7Le3faW22E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TV_qMkj6W24h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lc4A0iMTW26u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnYM0vA1W283"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2UQeLY3W3Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWUHe7csW4Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wd48FRArW4LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6j8t7MLWW4Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KItEngufW4RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CyMGU4B-W4f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "\n",
        "\n",
        "# 1. Using the gutenberg corpus in NLTK list all available file identifiers.\n",
        "from nltk.corpus import gutenberg\n",
        "file_ids = gutenberg.fileids()\n",
        "file_ids\n",
        "# . Calculate the avg word length, avg sentence length (in words) and lexical diversity for \"Moby Dick\" by Herman Melville using gutenberg corpus.\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
        "moby_dick_sentences = gutenberg.sents('melville-moby_dick.txt')\n",
        "\n",
        "word_lengths = [len(word) for word in moby_dick if word.isalpha()]\n",
        "avg_word_length = sum(word_lengths) / len(word_lengths)\n",
        "\n",
        "sentence_lengths = [len(sentence) for sentence in moby_dick_sentences]\n",
        "avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)\n",
        "\n",
        "lexical_diversity = len(set(moby_dick)) / len(moby_dick)\n",
        "\n",
        "avg_word_length, avg_sentence_length, lexical_diversity\n",
        "# 3. Uing the brown corpus find the most frequent word in the news category.\n",
        "from nltk.corpus import brown\n",
        "news_words = brown.words(categories='news')\n",
        "fdist_news = FreqDist(news_words)\n",
        "most_frequent_word = fdist_news.max()\n",
        "most_frequent_word"
      ],
      "metadata": {
        "id": "F5xVnMaV0l0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pVvYYEnW6XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBsbvWoeW6Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MGI9X8hW6cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwkZLAI-W6fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ByvelrVW6mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "akqJInU5W6pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjzpA28GW6yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0W6rCcMEW60v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pV9hqlrAW63D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gz0qqiwoW65M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rA8HjP6yW67e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3Nt8zwRW6-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter\n",
        "# Use the inaugural address corpus to find the total number of words and the total number of unique words in the inaugural addresses delivered in the 21st century\n",
        "addresses_21st_century = inaugural.fileids()[39:]\n",
        "\n",
        "total_words = sum(len(inaugural.words(fileid)) for fileid in addresses_21st_century)\n",
        "unique_words = set(word.lower() for fileid in addresses_21st_century for word in inaugural.words(fileid))\n",
        "\n",
        "total_words, len(unique_words)\n",
        "# Write a Python program to find the frequency distribution of the words \"democracy\", \"freedom\", \"liberty\", and \"equality\" in all inaugural addresses using NLTK.\n",
        "from nltk import FreqDist\n",
        "\n",
        "target_words = [\"democracy\", \"freedom\", \"liberty\", \"equality\"]\n",
        "\n",
        "fdist = FreqDist(word.lower() for fileid in inaugural.fileids() for word in inaugural.words(fileid))\n",
        "\n",
        "freq_target_words = {word: fdist[word] for word in target_words}\n",
        "freq_target_words\n",
        "# Write a Python program to display the 5 most common words in the text of \"Sense and Sensibility\" by Jane Austen using the Gutenberg Corpus.\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "sense_and_sensibility = gutenberg.words('austen-sense.txt')\n",
        "\n",
        "fdist_sense = FreqDist(word.lower() for word in sense_and_sensibility)\n",
        "\n",
        "fdist_sense.most_common(5)\n"
      ],
      "metadata": {
        "id": "KOJSR31e0_JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NIwweMDW7r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXjLeLqLW7t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEj2LyckW7wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DsCdBoGyW7yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1MlfbefCW70u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfQf0Rx_W72s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzcLfYEPW74r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95o6eWDWW76z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XmBCfaVW79F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6aVNIdc9W7_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIvWvJmiW8BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qGD8BVmwW8D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-JVm5btW8GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wqhgE4WW8I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CieUcLcHW8LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNLH8JxlW8Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter\n",
        "\n",
        "# Write a Python program to download the text of \"Pride and Prejudice\" by Jane Austen from Project Gutenberg, tokenize the text, and display the first 10 tokens.\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "response = urllib.request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens[:10])\n",
        "\n",
        "\n",
        "# Using NLTK, write a function that takes a URL as input, fetches the raw text from the webpage, and returns the number of words in the text.\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def count_words_from_url(url):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    web_content = response.read().decode('utf-8')\n",
        "    soup = BeautifulSoup(web_content, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return len(words)\n",
        "\n",
        "url = \"https://www.nltk.org/book/ch02.html\"\n",
        "print(count_words_from_url(url))\n",
        "\n",
        "\n",
        "# Explain how to remove HTML tags from a web page's content using Python and NLTK. Provide a code example that fetches a web page, removes HTML tags, and prints the cleaned text.\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_html_from_url(url):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    web_content = response.read().decode('utf-8')\n",
        "    soup = BeautifulSoup(web_content, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "url = \"https://www.nltk.org\"\n",
        "cleaned_text = clean_html_from_url(url)\n",
        "print(cleaned_text[:500])\n"
      ],
      "metadata": {
        "id": "zaI3UYhC2cFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PmMVVQ_PW9Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ae02gcVsW9Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3piyIlwW9Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wc-4z1zWW9bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4C7wJPsW9df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WgWOlJoW9f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8wuhjrDW9iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVcYoi3iW9kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQlKIIrEW9mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QD3fI9EDW9o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V79TbXWwW9rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Soyk_hT7W9tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5gElK7UYW9ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLupfklcW9xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7OP_uAJaW907"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter\n",
        "\n",
        "# Explain the difference between assigning a list to a new variable using direct assignment (=) and using the copy() method. Provide code examples to illustrate the difference.\n",
        "\n",
        "list1 = [1, 2, 3]\n",
        "list2 = list1\n",
        "list3 = list1.copy()\n",
        "\n",
        "list2[0] = 99\n",
        "list3[0] = 88\n",
        "\n",
        "print(\"list1:\", list1)\n",
        "print(\"list2:\", list2)\n",
        "print(\"list3:\", list3)\n",
        "\n",
        "# Write a function extract_nouns(text) that takes a text string as input and returns a list of all nouns in the text. Use NLTK's part-of-speech tagging for this task.\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def extract_nouns(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    return [word for word, tag in pos_tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog in the park.\"\n",
        "print(extract_nouns(text))\n",
        "\n",
        "# Demonstrate how to use list comprehension to create a list of the lengths of each word in a given sentence.\n",
        "sentence = \"This is a sample sentence\"\n",
        "word_lengths = [len(word) for word in sentence.split()]\n",
        "print(word_lengths)\n"
      ],
      "metadata": {
        "id": "1d1Jpox52z1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFyI2AVVW-sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XTG3flKeW-ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YL3uQHuW-xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7we_ULyW-zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNFczJjOW-1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqm_Ce8iW-37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ye4cz7bdW-5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKAc4r52W-8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaOiRTOgW--z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vw0kEum9W_Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10-MBVloW_D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAXtt34hW_GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ik0eTxkzW_Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfXkg9wGW_L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('names')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('all')\n",
        "# Write a Python program using NLTK to perform part-of-speech tagging on the sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize and perform POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# Using NLTK, write a function that takes a list of sentences and returns a list of part-of-speech tagged sentences.\n",
        "def pos_tag_sentences(sentences):\n",
        "    tagged_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tagged_sentences.append(nltk.pos_tag(tokens))\n",
        "\n",
        "    return tagged_sentences\n",
        "\n",
        "# Test the function with a list of sentences\n",
        "sentences = [\"The quick brown fox jumps over the lazy dog.\", \"NLTK is a leading toolkit for text processing.\"]\n",
        "result = pos_tag_sentences(sentences)\n",
        "for sentence, tags in zip(sentences, result):\n",
        "    print(f\"Sentence: {sentence}\\nPOS Tags: {tags}\\n\")\n",
        "\n",
        "\n",
        "# Explain how to map the Penn Treebank POS tags to the Universal POS tags using NLTK. Provide a code example that tags a sentence and maps the tags accordingly.\n",
        "# Define a mapping function\n",
        "def map_penn_to_universal(tag):\n",
        "    penn_to_universal = {\n",
        "        'NN': 'NOUN', 'NNS': 'NOUN', 'NNPS': 'NOUN', 'NNP': 'NOUN',\n",
        "        'VB': 'VERB', 'VBD': 'VERB', 'VBG': 'VERB', 'VBN': 'VERB', 'VBP': 'VERB', 'VBZ': 'VERB',\n",
        "        'JJ': 'ADJ', 'JJR': 'ADJ', 'JJS': 'ADJ',\n",
        "        'RB': 'ADV', 'RBR': 'ADV', 'RBS': 'ADV',\n",
        "        'DT': 'DET', 'PRP': 'PRON', 'PRP$': 'PRON', 'POS': 'PRON',\n",
        "        'IN': 'ADP', 'CC': 'CONJ', 'CD': 'NUM', 'UH': 'INTJ', 'WP': 'PRON', 'WP$': 'PRON'\n",
        "    }\n",
        "    return penn_to_universal.get(tag, 'X')  # Default 'X' for unknown tags\n",
        "\n",
        "# Example of POS tagging and mapping\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Map Penn Treebank tags to Universal POS tags\n",
        "universal_tags = [(word, map_penn_to_universal(tag)) for word, tag in pos_tags]\n",
        "print(\"Mapped Universal POS Tags:\", universal_tags)\n",
        "\n"
      ],
      "metadata": {
        "id": "fCobEo4S3OKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-utJRhLW_4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IaC8mOyJW_6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Oo35sQPW_8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bK7YJTntW__K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HlrECh9SXABU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqX7GamzXAD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ho9cvGjvXAGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cun3IL9QXAIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNCaCfdqXAKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAZhfVq2XAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDa7QZH4XAO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Jl93sJvXARZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTcLROT0XATi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkPSb5UgXAWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kc-yWM5zXAZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')\n",
        "# Using the names corpus in NLTK, build a gender classifier that predicts whether a name is male or female based on the last letter of the name. Evaluate its accuracy.\n",
        "from nltk.corpus import names\n",
        "import random\n",
        "\n",
        "# Prepare the training data using last letter of names\n",
        "def gender_features(name):\n",
        "    return {'last_letter': name[-1]}\n",
        "\n",
        "# Create training and test datasets\n",
        "names_data = [(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]\n",
        "random.shuffle(names_data)\n",
        "\n",
        "train_data = [(gender_features(name), gender) for (name, gender) in names_data[:int(len(names_data) * 0.8)]]\n",
        "test_data = [(gender_features(name), gender) for (name, gender) in names_data[int(len(names_data) * 0.8):]]\n",
        "\n",
        "# Train the classifier\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = nltk.classify.accuracy(classifier, test_data)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# Enhance the gender classifier by including features such as the first letter and the length of the name. Evaluate if these features improve the classifier's accuracy.\n",
        "# Enhanced feature extractor: first letter and name length\n",
        "def enhanced_gender_features(name):\n",
        "    return {\n",
        "        'last_letter': name[-1],\n",
        "        'first_letter': name[0],\n",
        "        'length': len(name)\n",
        "    }\n",
        "\n",
        "# Train the enhanced classifier\n",
        "train_data_enhanced = [(enhanced_gender_features(name), gender) for (name, gender) in names_data[:int(len(names_data) * 0.8)]]\n",
        "test_data_enhanced = [(enhanced_gender_features(name), gender) for (name, gender) in names_data[int(len(names_data) * 0.8):]]\n",
        "\n",
        "# Train the classifier\n",
        "enhanced_classifier = nltk.NaiveBayesClassifier.train(train_data_enhanced)\n",
        "\n",
        "# Evaluate the enhanced classifier\n",
        "enhanced_accuracy = nltk.classify.accuracy(enhanced_classifier, test_data_enhanced)\n",
        "print(f\"Enhanced Accuracy: {enhanced_accuracy}\")\n",
        "\n",
        "# Using the movie_reviews corpus in NLTK, build a document classifier to categorize movie reviews as positive or negative. Evaluate its performance.\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Feature extractor: bag of words\n",
        "def extract_features(words):\n",
        "    return {word: True for word in words}\n",
        "\n",
        "# Prepare the dataset\n",
        "positive_reviews = movie_reviews.categories('pos')\n",
        "negative_reviews = movie_reviews.categories('neg')\n",
        "\n",
        "positive_data = [(extract_features(movie_reviews.words(fileid)), 'pos') for fileid in movie_reviews.fileids('pos')]\n",
        "negative_data = [(extract_features(movie_reviews.words(fileid)), 'neg') for fileid in movie_reviews.fileids('neg')]\n",
        "\n",
        "# Train-test split\n",
        "train_data = positive_data[:int(len(positive_data) * 0.8)] + negative_data[:int(len(negative_data) * 0.8)]\n",
        "test_data = positive_data[int(len(positive_data) * 0.8):] + negative_data[int(len(negative_data) * 0.8):]\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "movie_classifier = nltk.NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "# Evaluate the classifier\n",
        "movie_accuracy = nltk.classify.accuracy(movie_classifier, test_data)\n",
        "print(f\"Movie Review Classifier Accuracy: {movie_accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a1JSWqXT3tOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quHgqb0JXBPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Vo1npYEXBRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cR6aoUXQXBT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCQwoPxqXBWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSBlzt3UXBYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5zGW1LLVXBbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f6hNwzdKXBd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pf-9HSrNXBgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6CbOPTWXBiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ofoJ4mhXBlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7zFxCgbXBnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scVGciIzXBqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xss7DPnXBsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9k6tV5BMXBus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIS5sPWsXBwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WE2p2ndTXBz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\tWrite a Python program using NLTK to extract named entities from the sentence: \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "# 1. Extract named entities from the sentence\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tags = pos_tag(tokens)\n",
        "tree = ne_chunk(tags)\n",
        "\n",
        "named_entities = []\n",
        "for subtree in tree:\n",
        "    if isinstance(subtree, nltk.Tree):\n",
        "        named_entities.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "print(named_entities)\n",
        "\n",
        "\n",
        "\n",
        "# Using NLTK, write a function that takes a list of sentences and returns a list of named entities found in each sentence.\n",
        "# 2. Function to extract named entities from a list of sentences\n",
        "def extract_named_entities_from_sentences(sentences):\n",
        "    named_entities_list = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tags = pos_tag(tokens)\n",
        "        tree = ne_chunk(tags)\n",
        "        named_entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, nltk.Tree):\n",
        "                named_entities.append(\" \".join([word for word, tag in subtree]))\n",
        "        named_entities_list.append(named_entities)\n",
        "    return named_entities_list\n",
        "\n",
        "sentences = [\"Apple Inc. is looking at buying U.K. startup for $1 billion.\",\n",
        "             \"Barack Obama visited New York City last week.\"]\n",
        "print(extract_named_entities_from_sentences(sentences))\n",
        "\n",
        "\n",
        "\n",
        "# Write a Python program that uses NLTK to extract and display all noun phrases from a given text.\n",
        "# 3. Extract and display all noun phrases from a given text\n",
        "from nltk import RegexpParser\n",
        "\n",
        "def extract_noun_phrases(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "    parser = RegexpParser(grammar)\n",
        "    tree = parser.parse(tags)\n",
        "\n",
        "    noun_phrases = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree) and subtree.label() == 'NP':\n",
        "            noun_phrases.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "    return noun_phrases\n",
        "\n",
        "text = \"The quick brown fox jumped over the lazy dog.\"\n",
        "print(extract_noun_phrases(text))\n",
        "\n"
      ],
      "metadata": {
        "id": "jxVUWptO4M8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oiy4yP4-XCiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n102KKycXCkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAQXiFV5XCnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bnkVZt6rXCpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7VX89JXXCrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqCAe3naXCtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6JArWb3KXCwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1pdh7XssXCyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hKPXubwgXC0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0VbrN10XC3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hy7JfA3PXC5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGGgrjrJXC7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFQD5GBMXC93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8vXesBcXDAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oXL2Ob_GXDCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmgYG7rjXDGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "# Write a Python program using NLTK to define a context-free grammar (CFG) that can parse simple sentences like \"The cat sat on the mat.\" Use this grammar to generate the parse tree for the sentence.\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# Define a context-free grammar (without comments)\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det N\n",
        "    VP -> V PP | V\n",
        "    PP -> P NP\n",
        "    Det -> 'the'\n",
        "    N -> 'cat' | 'mat'\n",
        "    V -> 'sat'\n",
        "    P -> 'on'\n",
        "\"\"\")\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"the cat sat on the mat\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "tokens = sentence.split()\n",
        "\n",
        "# Create a parser using the defined grammar\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "# Parse the sentence and generate the parse tree\n",
        "print(\"Parse Tree:\")\n",
        "for tree in parser.parse(tokens):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n",
        "\n",
        "\n",
        "# Using NLTK, write a function that takes a sentence as input and returns all possible parse trees using a given CFG. Demonstrate this function with the sentence \"I saw the man with the telescope.\"\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# Define a CFG\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> Pronoun | Det N | Det N PP\n",
        "  VP -> V NP | V NP PP\n",
        "  PP -> P NP\n",
        "  Pronoun -> 'I' | 'he' | 'she'\n",
        "  Det -> 'the' | 'a'\n",
        "  N -> 'man' | 'telescope'\n",
        "  V -> 'saw'\n",
        "  P -> 'with'\n",
        "\"\"\")\n",
        "\n",
        "# Function to return all parse trees\n",
        "def parse_all_trees(sentence):\n",
        "    parser = nltk.ChartParser(grammar)\n",
        "    trees = list(parser.parse(sentence))\n",
        "    return trees\n",
        "\n",
        "# Demonstrating with \"I saw the man with the telescope.\"\n",
        "sentence = ['I', 'saw', 'the', 'man', 'with', 'the', 'telescope']\n",
        "trees = parse_all_trees(sentence)\n",
        "\n",
        "# Display all parse trees\n",
        "for tree in trees:\n",
        "    tree.pretty_print()\n",
        "\n",
        "# Write a Python program using NLTK to create a recursive descent parser for a given CFG. Parse the sentence \"She eats a sandwich.\" and display the parse tree.\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# Define a CFG\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> Pronoun\n",
        "  VP -> V NP\n",
        "  Pronoun -> 'She'\n",
        "  V -> 'eats'\n",
        "  NP -> Det N\n",
        "  Det -> 'a'\n",
        "  N -> 'sandwich'\n",
        "\"\"\")\n",
        "\n",
        "# Recursive Descent Parser using NLTK's Top-Down ChartParser\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "# Sentence to parse\n",
        "sentence = ['She', 'eats', 'a', 'sandwich']\n",
        "\n",
        "# Parse and print the parse tree\n",
        "for tree in parser.parse(sentence):\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "id": "yH8tMpzw4Zza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWUJvQaGXDvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KvQzQxw8XDxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XOStDka2XDzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxG4kD-LXD2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8Pe8Yk1XD4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQk_u3BkXD7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qBn4tSuCXD91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlCKzTh-XD_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R20QKr1gXECS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2c_4ou6jXEEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Sj9idiqXEGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMD31GjCXEIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lDbnTvrXEKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_OzOrY7XENL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpGMUVOpXEQk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}