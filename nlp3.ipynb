{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugbpo3OH-we4",
        "outputId": "b0809c70-eba8-4c99-d11d-4ebd01591105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3\n",
        "1.\tWrite a Python program to download the text of \"Pride and Prejudice\" by Jane Austen from Project Gutenberg, tokenize the text, and display the first 10 tokens.\n"
      ],
      "metadata": {
        "id": "ka9rsXMj_CUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "response = urllib.request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-i3fjUHB6cv",
        "outputId": "4b93d66c-3c16-493c-fbc8-044dc0802b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['*', '*', '*', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', '1342']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tUsing NLTK, write a function that takes a URL as input, fetches the raw text from the webpage, and returns the number of words in the text.\n"
      ],
      "metadata": {
        "id": "2zGlWon-_I_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def count_words_from_url(url):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    web_content = response.read().decode('utf-8')\n",
        "    soup = BeautifulSoup(web_content, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return len(words)\n",
        "\n",
        "url = \"https://www.nltk.org/book/ch02.html\"\n",
        "print(count_words_from_url(url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1ZCET1XB62n",
        "outputId": "f0a7cb9b-9365-4681-f2b5-8334c807dce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a88b35d3c348>:7: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(web_content, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tExplain how to remove HTML tags from a web page's content using Python and NLTK. Provide a code example that fetches a web page, removes HTML tags, and prints the cleaned text.\n"
      ],
      "metadata": {
        "id": "slAvXQtJ_J9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_html_from_url(url):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    web_content = response.read().decode('utf-8')\n",
        "    soup = BeautifulSoup(web_content, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "url = \"https://www.nltk.org\"\n",
        "cleaned_text = clean_html_from_url(url)\n",
        "print(cleaned_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGRWHAvcB8hZ",
        "outputId": "d8867b3e-7514-4037-cc6a-020a331ad102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NLTK :: Natural Language Toolkit\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NLTK\n",
            "\n",
            "\n",
            "\n",
            "Documentation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NLTK Documentation\n",
            "\n",
            "API Reference\n",
            "Example Usage\n",
            "Module Index\n",
            "Wiki\n",
            "FAQ\n",
            "Open Issues\n",
            "NLTK on GitHub\n",
            "\n",
            "Installation\n",
            "\n",
            "Installing NLTK\n",
            "Installing NLTK Data\n",
            "\n",
            "More\n",
            "\n",
            "Release Notes\n",
            "Contributing to NLTK\n",
            "NLTK Team\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Natural Language ToolkitÂ¶\n",
            "NLTK is a leading platform for building Python programs to work with human language data.\n",
            "It provides easy-to-use interfaces to over 50 corpora and lexical\n",
            "resources such as WordN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tWrite a Python program that reads a text file, tokenizes its content into sentences, and prints the number of sentences in the file.\n"
      ],
      "metadata": {
        "id": "gXwyXq9M_LHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "with open('file.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zW0y1U4B9Mo",
        "outputId": "bcaeb548-39d5-437b-b3d5-0975906a0b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tUsing regular expressions in Python, write a function that takes a list of words and returns a list of words that end with 'ing'.\n"
      ],
      "metadata": {
        "id": "5-z3ueIm_MFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_words_ending_with_ing(word_list):\n",
        "    return [word for word in word_list if re.search(r'ing$', word)]\n",
        "\n",
        "words = [\"running\", \"swimming\", \"fly\", \"jumping\"]\n",
        "print(find_words_ending_with_ing(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW3xBvy2B9qK",
        "outputId": "46561c64-1ba2-4090-a18a-024113ea5a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'swimming', 'jumping']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\tDescribe how to normalize text by converting it to lowercase and removing punctuation using NLTK. Provide a code example that processes a given sentence.\n"
      ],
      "metadata": {
        "id": "qu9wbu3W_M8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    return [word for word in word_tokenize(text) if word not in string.punctuation]\n",
        "\n",
        "sentence = \"Hello, World! This is a test.\"\n",
        "print(normalize_text(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2KEHnF9B-J_",
        "outputId": "98218bf2-b8e3-483b-8c0d-62b2ffdf75b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'this', 'is', 'a', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\tUsing NLTK's PorterStemmer, write a function that takes a list of words and returns a list of their stemmed forms.\n"
      ],
      "metadata": {
        "id": "xVMVzoJP_N3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_words(word_list):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in word_list]\n",
        "\n",
        "words = [\"running\", \"ran\", \"eating\", \"eats\"]\n",
        "print(stem_words(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxwSKOMqB-oZ",
        "outputId": "18c6857f-9487-4dcf-8528-70b852942408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'ran', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.\tExplain the difference between stemming and lemmatization in text processing. Provide code examples using NLTK to demonstrate both processes on the word 'running'.\n"
      ],
      "metadata": {
        "id": "a0B1CmjO_Oo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "print(\"Stemmed word:\", stemmer.stem(\"running\"))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Lemmatized word:\", lemmatizer.lemmatize(\"running\", pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEEUxK1LB_Ko",
        "outputId": "84468e12-caa6-4a01-d36d-6ee1dfa13ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word: run\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized word: run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\tWrite a Python program to count the number of vowels (a, e, i, o, u) in a given text.\n"
      ],
      "metadata": {
        "id": "qz8n5dJ2_PeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_vowels(text):\n",
        "    vowels = \"aeiou\"\n",
        "    return sum(1 for char in text.lower() if char in vowels)\n",
        "\n",
        "text = \"Hello, how are you today?\"\n",
        "print(count_vowels(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfbE-4mfB_m4",
        "outputId": "391c130c-55e3-4448-da65-0fa25b7e3d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.\tWrite a Python program to tokenize the text into words and filter out words that contain digits.\n"
      ],
      "metadata": {
        "id": "DnW416lb_QZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def filter_words_with_digits(text):\n",
        "    words = word_tokenize(text)\n",
        "    return [word for word in words if not any(char.isdigit() for char in word)]\n",
        "\n",
        "text = \"This is a test 123 and 45test\"\n",
        "print(filter_words_with_digits(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZyzqtLYCABd",
        "outputId": "7247c73a-6fdc-45d5-872a-bcd9c4cf1d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'test', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.\tWrite a program to read text from a file and display all the words starting with a capital letter.\n"
      ],
      "metadata": {
        "id": "0R83ZPqZ_RtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def words_starting_with_capital(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    return re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "\n",
        "print(words_starting_with_capital('file.txt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ4wQl8yCAdv",
        "outputId": "ce2bfc8d-1833-4f97-aeb1-e60d84a54e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'Shivansi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.\tWrite a Python program to identify and display all the email addresses from a given text.\n"
      ],
      "metadata": {
        "id": "7qnMqEDZ_SrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_email_addresses(text):\n",
        "    return re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', text)\n",
        "\n",
        "text = \"Please contact us at support@example.com or admin@domain.org\"\n",
        "print(find_email_addresses(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjWhEDmPCA5f",
        "outputId": "ebf1cc07-9a0c-4d4c-a260-97dad2caeff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@example.com', 'admin@domain.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.\tWrite a program to tokenize a text into sentences and display only those sentences containing more than 10 words.\n"
      ],
      "metadata": {
        "id": "aVRzRyHa_YEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def filter_long_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return [sentence for sentence in sentences if len(word_tokenize(sentence)) > 10]\n",
        "\n",
        "text = \"This is a short sentence. This is a much longer sentence with more than ten words in it.\"\n",
        "print(filter_long_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9XfS4WMCBZw",
        "outputId": "cc5caa1f-09f3-4417-dda9-513bca71c838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a much longer sentence with more than ten words in it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.\tWrite a Python program that takes a sentence as input and returns all words that are at least 5 characters long.\n"
      ],
      "metadata": {
        "id": "eFJXXfqZ_Y7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def filter_long_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    return [word for word in words if len(word) >= 5]\n",
        "\n",
        "sentence = \"This is a simple sentence with a few longer words.\"\n",
        "print(filter_long_words(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abj9isL6CCf3",
        "outputId": "75baaf65-80c1-4252-c0b2-3b33d9140214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'sentence', 'longer', 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.\tWrite a Python program using regular expressions to find all words starting with 'un' in a given text."
      ],
      "metadata": {
        "id": "Rx7Zwu5Q_Zuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_words_starting_with_un(text):\n",
        "    return re.findall(r'\\bun\\w*', text)\n",
        "\n",
        "text = \"The unaccountable events unfolded in an unusual manner.\"\n",
        "print(find_words_starting_with_un(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzjc17SCCDEx",
        "outputId": "a341c014-5fda-4eeb-b81e-cdc92e12d6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['unaccountable', 'unfolded', 'unusual']\n"
          ]
        }
      ]
    }
  ]
}