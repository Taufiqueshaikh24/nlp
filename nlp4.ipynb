{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugbpo3OH-we4",
        "outputId": "b0809c70-eba8-4c99-d11d-4ebd01591105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4\n",
        "1.\tExplain the difference between assigning a list to a new variable using direct assignment (=) and using the copy() method. Provide code examples to illustrate the difference.\n"
      ],
      "metadata": {
        "id": "fkHpPoJj_cpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [1, 2, 3]\n",
        "list2 = list1\n",
        "list3 = list1.copy()\n",
        "\n",
        "list2[0] = 99\n",
        "list3[0] = 88\n",
        "\n",
        "print(\"list1:\", list1)\n",
        "print(\"list2:\", list2)\n",
        "print(\"list3:\", list3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAhOdlqgCDqn",
        "outputId": "5d768c00-40db-48ce-b589-25d93cc9b245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list1: [99, 2, 3]\n",
            "list2: [99, 2, 3]\n",
            "list3: [88, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tWrite a function extract_nouns(text) that takes a text string as input and returns a list of all nouns in the text. Use NLTK's part-of-speech tagging for this task.\n"
      ],
      "metadata": {
        "id": "-WAJo52H_hac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def extract_nouns(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    return [word for word, tag in pos_tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog in the park.\"\n",
        "print(extract_nouns(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQvYlTluCEEV",
        "outputId": "2da04e5c-196e-49ea-fc24-eb14ce5a54c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown', 'fox', 'dog', 'park']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tDemonstrate how to use list comprehension to create a list of the lengths of each word in a given sentence.\n"
      ],
      "metadata": {
        "id": "-YaOb6lt_mX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This is a sample sentence\"\n",
        "word_lengths = [len(word) for word in sentence.split()]\n",
        "print(word_lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk13yaJPCEk4",
        "outputId": "fe45622c-1d9f-45ba-8727-eee116d2d8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 2, 1, 6, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tWrite a function word_frequency(text) that takes a text string and returns a dictionary with words as keys and their frequencies as values.\n"
      ],
      "metadata": {
        "id": "-9hlMFIT_nXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def word_frequency(text):\n",
        "    words = text.split()\n",
        "    return dict(Counter(words))\n",
        "\n",
        "text = \"apple banana apple cherry banana apple\"\n",
        "print(word_frequency(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ayPu9hgCFDH",
        "outputId": "4ad785b7-6d17-4aa5-c573-e280590e6285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'apple': 3, 'banana': 2, 'cherry': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tExplain the concept of variable scope in Python with an example demonstrating the difference between local and global variables.\n"
      ],
      "metadata": {
        "id": "X3gzXeAm_qkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 10\n",
        "\n",
        "def local_scope_example():\n",
        "    x = 20\n",
        "    print(\"Inside function:\", x)\n",
        "\n",
        "local_scope_example()\n",
        "print(\"Outside function:\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rQ8pSsLCFig",
        "outputId": "3ecea1d7-fcee-4536-a768-b386e24b5a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside function: 20\n",
            "Outside function: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\tWrite a Python program that reads a text file and counts the number of lines, words, and characters in the file.\n"
      ],
      "metadata": {
        "id": "quLJTaaH_rh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def file_statistics(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        num_lines = len(lines)\n",
        "        num_words = sum(len(line.split()) for line in lines)\n",
        "        num_chars = sum(len(line) for line in lines)\n",
        "    return num_lines, num_words, num_chars\n",
        "\n",
        "file_path = 'file.txt'\n",
        "print(file_statistics(file_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kvz5jH5CGEK",
        "outputId": "4b698558-8664-4171-8da9-4742d9ab479f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\tDescribe how to handle exceptions in Python with a try-except block. Provide an example that handles a division by zero error.\n"
      ],
      "metadata": {
        "id": "iXX_Sl_5_sf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    result = 10 / 0\n",
        "except ZeroDivisionError:\n",
        "    print(\"Error: Division by zero\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDXGeVzICGip",
        "outputId": "973e57a9-206f-46cb-f841-5ef5d6766a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.\tWrite a function remove_stopwords(text) that removes common English stopwords from a given text using NLTK's stopwords corpus.\n"
      ],
      "metadata": {
        "id": "ht7VOMC6_tkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    return [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "text = \"This is an example of text containing stopwords.\"\n",
        "print(remove_stopwords(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk1os4iCCG_X",
        "outputId": "d4dac989-0511-43df-8a91-829749ff4e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'text', 'containing', 'stopwords', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\tWrite a Python program to extract and print all words from a text that start with a capital letter using regular expressions.\n"
      ],
      "metadata": {
        "id": "vaGQEaQM_ufZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def words_starting_with_capital(text):\n",
        "    return re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "\n",
        "text = \"The quick Brown fox Jumps over the Lazy Dog.\"\n",
        "print(words_starting_with_capital(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOulia5eCHZ2",
        "outputId": "548715fc-850d-4696-96ce-81fe524ee295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Brown', 'Jumps', 'Lazy', 'Dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.\tWrite a Python program to tokenize a given text into words and count how many words are exactly 5 characters long.\n"
      ],
      "metadata": {
        "id": "sjlj3hp9_vgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_5_char_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    return len([word for word in words if len(word) == 5])\n",
        "\n",
        "text = \"Hello world, this is an example.\"\n",
        "print(count_5_char_words(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juThlIevCH3X",
        "outputId": "588a8e20-0bcd-45b2-95aa-1f1c6d0e4576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.\tWrite a Python program to use regular expressions to find all email addresses in a given string.\n"
      ],
      "metadata": {
        "id": "72AkfMyf_wbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_email_addresses(text):\n",
        "    return re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', text)\n",
        "\n",
        "text = \"Please contact us at support@example.com or admin@domain.org.\"\n",
        "print(find_email_addresses(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNmovfGcCISO",
        "outputId": "f566b25d-1cfa-46e4-8d4a-e0c340974596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@example.com', 'admin@domain.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.\tWrite a Python program to tokenize a sentence into words and then sort the words alphabetically.\n",
        "\n"
      ],
      "metadata": {
        "id": "MYLgZJhc_y3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_words_alphabetically(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return sorted(words)\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "print(sort_words_alphabetically(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5JqnRTqCI4F",
        "outputId": "586d9da1-b63d-41d3-8db9-cea490f6a67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.', 'The', 'brown', 'dog', 'fox', 'jumps', 'lazy', 'over', 'quick', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.\tWrite a Python program to calculate and print the lexical diversity of a given text.\n"
      ],
      "metadata": {
        "id": "dKJQNfbL_0Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexical_diversity(text):\n",
        "    words = word_tokenize(text)\n",
        "    return len(set(words)) / len(words)\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "print(lexical_diversity(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbT3kgSCJVX",
        "outputId": "12cd2975-da12-40f7-df50-d2488746fca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.\tWrite a Python program that takes a sentence as input and prints the words in reverse order.\n"
      ],
      "metadata": {
        "id": "mq5Sp4cn_1Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_words(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return ' '.join(reversed(words))\n",
        "\n",
        "sentence = \"This is a test sentence.\"\n",
        "print(reverse_words(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXqe-OF7CJzX",
        "outputId": "c70d9009-4764-40c9-ea6b-901339858390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". sentence test a is This\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.\tWrite a Python program to find and print all words in a text that contain at least one digit."
      ],
      "metadata": {
        "id": "hTB6nBqt_2I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def words_with_digit(text):\n",
        "    return [word for word in word_tokenize(text) if any(char.isdigit() for char in word)]\n",
        "\n",
        "text = \"There are gf3a cats and bv5 dogs in the house.\"\n",
        "print(words_with_digit(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5M1LYICCKXO",
        "outputId": "260e8096-bd0f-4a52-d6c1-e45a37ead82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gf3a', 'bv5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdO3aRiPGRvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}