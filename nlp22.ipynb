{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugbpo3OH-we4",
        "outputId": "b0809c70-eba8-4c99-d11d-4ebd01591105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2\n",
        "\n",
        "1.\tUse the inaugural address corpus to find the total number of words and the total number of unique words in the inaugural addresses delivered in the 21st century."
      ],
      "metadata": {
        "id": "3AlEZdAx-3PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "addresses_21st_century = inaugural.fileids()[39:]\n",
        "\n",
        "total_words = sum(len(inaugural.words(fileid)) for fileid in addresses_21st_century)\n",
        "unique_words = set(word.lower() for fileid in addresses_21st_century for word in inaugural.words(fileid))\n",
        "\n",
        "total_words, len(unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFNyviG_AYN2",
        "outputId": "1fb221f4-e9f1-4627-c8c0-9c633ce26138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43821, 4514)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tWrite a Python program to find the frequency distribution of the words \"democracy\", \"freedom\", \"liberty\", and \"equality\" in all inaugural addresses using NLTK.\n"
      ],
      "metadata": {
        "id": "zOhHYmFj-870"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "target_words = [\"democracy\", \"freedom\", \"liberty\", \"equality\"]\n",
        "\n",
        "fdist = FreqDist(word.lower() for fileid in inaugural.fileids() for word in inaugural.words(fileid))\n",
        "\n",
        "freq_target_words = {word: fdist[word] for word in target_words}\n",
        "freq_target_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ShiB8EBBRaA",
        "outputId": "428265de-6e04-43d8-a6e4-f35977fdcb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'democracy': 71, 'freedom': 189, 'liberty': 123, 'equality': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tWrite a Python program to display the 5 most common words in the text of \"Sense and Sensibility\" by Jane Austen using the Gutenberg Corpus.\n"
      ],
      "metadata": {
        "id": "CG5KUBF9-93z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "sense_and_sensibility = gutenberg.words('austen-sense.txt')\n",
        "\n",
        "fdist_sense = FreqDist(word.lower() for word in sense_and_sensibility)\n",
        "\n",
        "fdist_sense.most_common(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9knKopLMBUUw",
        "outputId": "eefab2d6-29be-48b6-a90c-63d4de9f10c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 9397), ('to', 4116), ('the', 4105), ('.', 3975), ('of', 3572)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tGenerate a conditional frequency distribution of modal verbs ('can', 'could', 'may', 'might', 'must', 'will') across the categories of the Brown Corpus.\n"
      ],
      "metadata": {
        "id": "rWA0HzPU--0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (category, word.lower()) for category in brown.categories() for word in brown.words(categories=category)\n",
        ")\n",
        "\n",
        "filtered_cfd = {category: {word: count for word, count in cfd[category].items() if word in modal_verbs} for category in cfd}\n",
        "\n",
        "filtered_cfd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fodwlKH_BZEH",
        "outputId": "0f2d5b9a-6a6d-4e3f-aa79-a9d52e14b1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adventure': {'could': 154,\n",
              "  'must': 27,\n",
              "  'might': 59,\n",
              "  'can': 48,\n",
              "  'will': 51,\n",
              "  'may': 7},\n",
              " 'belles_lettres': {'may': 221,\n",
              "  'will': 246,\n",
              "  'could': 216,\n",
              "  'must': 171,\n",
              "  'can': 249,\n",
              "  'might': 113},\n",
              " 'editorial': {'may': 79,\n",
              "  'can': 124,\n",
              "  'will': 235,\n",
              "  'must': 55,\n",
              "  'could': 57,\n",
              "  'might': 39},\n",
              " 'fiction': {'can': 39,\n",
              "  'might': 44,\n",
              "  'could': 168,\n",
              "  'must': 55,\n",
              "  'will': 56,\n",
              "  'may': 10},\n",
              " 'government': {'may': 179,\n",
              "  'can': 119,\n",
              "  'must': 102,\n",
              "  'will': 244,\n",
              "  'might': 13,\n",
              "  'could': 38},\n",
              " 'hobbies': {'could': 59,\n",
              "  'can': 276,\n",
              "  'will': 269,\n",
              "  'must': 84,\n",
              "  'may': 143,\n",
              "  'might': 22},\n",
              " 'humor': {'might': 8,\n",
              "  'can': 17,\n",
              "  'could': 33,\n",
              "  'may': 8,\n",
              "  'will': 13,\n",
              "  'must': 9},\n",
              " 'learned': {'will': 340,\n",
              "  'may': 336,\n",
              "  'might': 128,\n",
              "  'could': 159,\n",
              "  'must': 203,\n",
              "  'can': 367},\n",
              " 'lore': {'could': 142,\n",
              "  'can': 170,\n",
              "  'might': 50,\n",
              "  'may': 170,\n",
              "  'must': 96,\n",
              "  'will': 178},\n",
              " 'mystery': {'could': 145,\n",
              "  'must': 31,\n",
              "  'can': 45,\n",
              "  'might': 57,\n",
              "  'will': 25,\n",
              "  'may': 15},\n",
              " 'news': {'may': 93,\n",
              "  'might': 38,\n",
              "  'will': 389,\n",
              "  'must': 53,\n",
              "  'can': 94,\n",
              "  'could': 87},\n",
              " 'religion': {'must': 54,\n",
              "  'may': 79,\n",
              "  'could': 59,\n",
              "  'can': 84,\n",
              "  'might': 12,\n",
              "  'will': 72},\n",
              " 'reviews': {'can': 45,\n",
              "  'could': 40,\n",
              "  'will': 61,\n",
              "  'might': 26,\n",
              "  'may': 47,\n",
              "  'must': 19},\n",
              " 'romance': {'could': 195,\n",
              "  'might': 51,\n",
              "  'can': 79,\n",
              "  'will': 49,\n",
              "  'must': 46,\n",
              "  'may': 11},\n",
              " 'science_fiction': {'could': 49,\n",
              "  'can': 16,\n",
              "  'may': 4,\n",
              "  'will': 17,\n",
              "  'must': 8,\n",
              "  'might': 12}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tWrite a Python program to identify the longest word in \"Moby Dick\" from the Gutenberg Corpus.\n"
      ],
      "metadata": {
        "id": "Nx6_RHMo-_uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
        "\n",
        "longest_word = max(moby_dick, key=len)\n",
        "longest_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r2nhQmUzBuT5",
        "outputId": "9dae019f-7f60-42b1-c1c7-311893bd3b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'uninterpenetratingly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\tUsing the Brown Corpus, calculate the frequency of the word \"government\" across all categories.\n"
      ],
      "metadata": {
        "id": "MdvLuqzH_Aht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "government_freq = sum(1 for word in brown.words() if word.lower() == 'government')\n",
        "government_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SG135OQBvf_",
        "outputId": "846dc39b-0b3d-49be-9d10-724631c1243b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "418"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\tWrite a Python program using the Reuters Corpus to find the number of documents categorized under \"crude\".\n"
      ],
      "metadata": {
        "id": "VZ9jEeKm_Bfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "crude_docs = [fileid for fileid in reuters.fileids() if 'crude' in reuters.categories(fileid)]\n",
        "\n",
        "len(crude_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2SVIoE0ByhP",
        "outputId": "6429299d-a4c5-4538-ff3d-8fcc09ee9ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "578"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdO3aRiPGRvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}