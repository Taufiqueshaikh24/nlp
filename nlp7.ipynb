{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.\tWrite a Python program using NLTK to extract named entities from the sentence: \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n"
      ],
      "metadata": {
        "id": "YICB0J67_3XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "6AwjmTq7BVbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLqEYdXV_zTb",
        "outputId": "b612119d-f715-4c80-d1bd-9c075a04da41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple', 'Inc.']\n"
          ]
        }
      ],
      "source": [
        "# 1. Extract named entities from the sentence\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tags = pos_tag(tokens)\n",
        "tree = ne_chunk(tags)\n",
        "\n",
        "named_entities = []\n",
        "for subtree in tree:\n",
        "    if isinstance(subtree, nltk.Tree):\n",
        "        named_entities.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "print(named_entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tUsing NLTK, write a function that takes a list of sentences and returns a list of named entities found in each sentence.\n"
      ],
      "metadata": {
        "id": "2_2tp7enBdL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Function to extract named entities from a list of sentences\n",
        "def extract_named_entities_from_sentences(sentences):\n",
        "    named_entities_list = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tags = pos_tag(tokens)\n",
        "        tree = ne_chunk(tags)\n",
        "        named_entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, nltk.Tree):\n",
        "                named_entities.append(\" \".join([word for word, tag in subtree]))\n",
        "        named_entities_list.append(named_entities)\n",
        "    return named_entities_list\n",
        "\n",
        "sentences = [\"Apple Inc. is looking at buying U.K. startup for $1 billion.\",\n",
        "             \"Barack Obama visited New York City last week.\"]\n",
        "print(extract_named_entities_from_sentences(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us46KfUxBeO6",
        "outputId": "46bb188e-bbc4-4835-e68e-746e6799bdff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Apple', 'Inc.'], ['Barack', 'Obama', 'New York City']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tWrite a Python program that uses NLTK to extract and display all noun phrases from a given text.\n"
      ],
      "metadata": {
        "id": "A-HdwL8GBeVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Extract and display all noun phrases from a given text\n",
        "from nltk import RegexpParser\n",
        "\n",
        "def extract_noun_phrases(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "    parser = RegexpParser(grammar)\n",
        "    tree = parser.parse(tags)\n",
        "\n",
        "    noun_phrases = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree) and subtree.label() == 'NP':\n",
        "            noun_phrases.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "    return noun_phrases\n",
        "\n",
        "text = \"The quick brown fox jumped over the lazy dog.\"\n",
        "print(extract_noun_phrases(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab6tAdkpBfS5",
        "outputId": "d9ee4bb2-83e6-47d7-fc91-89386276d4de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The quick brown fox', 'the lazy dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tUsing NLTK, write a program to perform chunking on the sentence: \"He reckons the current account deficit will narrow to only 8 billion.\" and display the chunked tree.\n"
      ],
      "metadata": {
        "id": "rFMREQOEBfXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Chunking on the sentence and display the chunked tree\n",
        "sentence = \"He reckons the current account deficit will narrow to only 8 billion.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "grammar = \"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN.*>}\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>*}\n",
        "\"\"\"\n",
        "parser = RegexpParser(grammar)\n",
        "tree = parser.parse(tags)\n",
        "\n",
        "tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QybWTTRuBgPZ",
        "outputId": "87c22c96-1832-4553-847a-64918bcb64a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              S                                                               \n",
            "   ___________________________________________|__________________________________________________________      \n",
            "  |       |      |      |     |       |       |                         VP                               |    \n",
            "  |       |      |      |     |       |       |        _________________|_____________________           |     \n",
            "  |       |      |      |     |       |       |       |                 NP                    NP         VP   \n",
            "  |       |      |      |     |       |       |       |         ________|__________           |          |     \n",
            "He/PRP will/MD to/TO only/RB 8/CD billion/CD ./. reckons/VBZ the/DT current/JJ account/NN deficit/NN narrow/VB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tWrite a Python function using NLTK that takes a sentence as input and returns all verb phrases (VP) present in the sentence.\n"
      ],
      "metadata": {
        "id": "UW9STIx6BgUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Function to extract all verb phrases (VP) from a sentence\n",
        "def extract_verb_phrases(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tags = pos_tag(tokens)\n",
        "\n",
        "    grammar = \"VP: {<VB.*><NP|PP|CLAUSE>*}\"\n",
        "    parser = RegexpParser(grammar)\n",
        "    tree = parser.parse(tags)\n",
        "\n",
        "    verb_phrases = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree) and subtree.label() == 'VP':\n",
        "            verb_phrases.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "    return verb_phrases\n",
        "\n",
        "sentence = \"She has been working hard on the project.\"\n",
        "print(extract_verb_phrases(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRbyJpNvBhqR",
        "outputId": "a2cd408e-0bd8-43fe-e3b9-e075f0b3f5de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['has', 'been', 'working']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\tWrite a Python program using NLTK to perform named entity recognition (NER) on a paragraph containing multiple sentences. Display the extracted named entities.\n"
      ],
      "metadata": {
        "id": "2gpcks1yBhvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Named entity recognition (NER) on a paragraph with multiple sentences\n",
        "paragraph = \"Apple Inc. is planning to acquire U.K.-based startup for $1 billion. Microsoft is also interested in the deal.\"\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "named_entities = []\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tags = pos_tag(tokens)\n",
        "    tree = ne_chunk(tags)\n",
        "\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree):\n",
        "            named_entities.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP21Q31mBiv6",
        "outputId": "7106ae0e-717b-486e-99c1-614fb80e47f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple', 'Inc.', 'Microsoft']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\tWrite a Python program to count the number of named entities of type GPE (Geopolitical Entity) in a given text.\n"
      ],
      "metadata": {
        "id": "x9-BpbFBBi0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Count the number of named entities of type GPE (Geopolitical Entity) in a given text\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "def count_GPE_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "    tree = ne_chunk(tags)\n",
        "\n",
        "    gpe_count = 0\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree):\n",
        "            label = subtree.label()\n",
        "            if label == 'GPE':  # GPE is used for Geopolitical Entity\n",
        "                gpe_count += 1\n",
        "\n",
        "    return gpe_count\n",
        "\n",
        "text = \"Barack Obama is from the United States, and he visited Canada.\"\n",
        "print(count_GPE_entities(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB6pcDnRBjoj",
        "outputId": "314e48e9-77bc-4773-d1ac-edc634244730"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.\tWrite a Python program to extract all organization names from a given text using NLTK's Named Entity Recognition (NER).\n"
      ],
      "metadata": {
        "id": "xMNsleuBBjta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "def extract_organizations(text):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    organizations = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Tokenize each sentence into words\n",
        "        words = word_tokenize(sentence)\n",
        "        # Part-of-speech tagging\n",
        "        pos_tags = pos_tag(words)\n",
        "        # Named Entity Recognition\n",
        "        tree = ne_chunk(pos_tags)\n",
        "\n",
        "        # Extract organizations from the tree\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, Tree):\n",
        "                if subtree.label() == 'ORGANIZATION':\n",
        "                    organization = \" \".join([word for word, pos in subtree.leaves()])\n",
        "                    organizations.append(organization)\n",
        "\n",
        "    return organizations\n",
        "\n",
        "text = \"\"\"\n",
        "Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\n",
        "Google LLC is an American multinational technology company that specializes in Internet-related services and products.\n",
        "Microsoft Corporation is an American multinational technology company with headquarters in Redmond, Washington.\n",
        "\"\"\"\n",
        "\n",
        "organizations = extract_organizations(text)\n",
        "\n",
        "print(\"Organizations found:\")\n",
        "for org in organizations:\n",
        "    print(org)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JrwZsCMBkgh",
        "outputId": "04322e4a-a480-4189-f892-37cd931ed080"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organizations found:\n",
            "Inc.\n",
            "LLC\n",
            "Corporation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\tWrite a Python program using NLTK to extract proper nouns (NNP) from a given sentence.\n"
      ],
      "metadata": {
        "id": "b3UEBiKpBkrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Extract proper nouns (NNP) from a given sentence\n",
        "def extract_proper_nouns(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tags = pos_tag(tokens)\n",
        "\n",
        "    proper_nouns = [word for word, tag in tags if tag == 'NNP']\n",
        "\n",
        "    return proper_nouns\n",
        "\n",
        "sentence = \"Barack Obama visited New York City last week.\"\n",
        "print(extract_proper_nouns(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13hGRnp7BlhB",
        "outputId": "3bcb9a23-cf3e-4513-a411-1a011f8eaf34"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barack', 'Obama', 'New', 'York', 'City']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.\tWrite a Python program to extract noun phrases from a given text using a custom chunking grammar.\n"
      ],
      "metadata": {
        "id": "QUqrTS7RBlly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Extract noun phrases from a given text using a custom chunking grammar\n",
        "from nltk import RegexpParser\n",
        "\n",
        "def extract_noun_phrases_custom_grammar(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"  # Define noun phrase grammar\n",
        "    parser = RegexpParser(grammar)\n",
        "    tree = parser.parse(tags)\n",
        "\n",
        "    noun_phrases = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree) and subtree.label() == 'NP':\n",
        "            noun_phrases.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "    return noun_phrases\n",
        "\n",
        "text = \"The quick brown fox jumped over the lazy dog.\"\n",
        "print(extract_noun_phrases_custom_grammar(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD7g2CujBoBq",
        "outputId": "7fc3791c-4e11-40e4-807c-e6c027b59bfd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The quick brown fox', 'the lazy dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.\tWrite a Python program to extract verb phrases (VP) from a given sentence using a custom chunking grammar.\n"
      ],
      "metadata": {
        "id": "jm9I9DnUBoIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Extract verb phrases (VP) from a given sentence using a custom chunking grammar\n",
        "def extract_verb_phrases_custom_grammar(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tags = pos_tag(tokens)\n",
        "\n",
        "    grammar = \"VP: {<VB.*><NP|PP|CLAUSE>*}\"  # Define verb phrase grammar\n",
        "    parser = RegexpParser(grammar)\n",
        "    tree = parser.parse(tags)\n",
        "\n",
        "    verb_phrases = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree) and subtree.label() == 'VP':\n",
        "            verb_phrases.append(\" \".join([word for word, tag in subtree]))\n",
        "\n",
        "    return verb_phrases\n",
        "\n",
        "sentence = \"She is writing a letter to her friend.\"\n",
        "print(extract_verb_phrases_custom_grammar(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3fTwXqnBqKo",
        "outputId": "9b1e9214-046d-42d8-dbaa-bd3ef77c362a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'writing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.\tWrite a Python program that extracts all named entities and classifies them into their respective categories (PERSON, ORGANIZATION, GPE, etc.).\n"
      ],
      "metadata": {
        "id": "86WfI7SBBqPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Extract all named entities and classify them into their respective categories (PERSON, ORGANIZATION, GPE, etc.)\n",
        "def classify_named_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "    tree = ne_chunk(tags)\n",
        "\n",
        "    classified_entities = {'PERSON': [], 'ORGANIZATION': [], 'GPE': []}\n",
        "\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.Tree):\n",
        "            label = subtree.label()\n",
        "            entity = \" \".join([word for word, tag in subtree])\n",
        "            if label == 'PERSON':\n",
        "                classified_entities['PERSON'].append(entity)\n",
        "            elif label == 'GPE':\n",
        "                classified_entities['GPE'].append(entity)\n",
        "            elif label == 'ORGANIZATION':\n",
        "                classified_entities['ORGANIZATION'].append(entity)\n",
        "\n",
        "    return classified_entities\n",
        "\n",
        "text = \"Apple Inc. and Microsoft collaborated with Barack Obama.\"\n",
        "print(classify_named_entities(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySr7AQi2Bq7y",
        "outputId": "f6e58e4b-7103-4d26-958a-0d1cb2b0958e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PERSON': ['Apple', 'Microsoft', 'Barack Obama'], 'ORGANIZATION': ['Inc.'], 'GPE': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.\tWrite a Python program to visualize named entities using ne_chunk from NLTK."
      ],
      "metadata": {
        "id": "YPpir6sfBrBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Visualize named entities using ne_chunk from NLTK\n",
        "from nltk import ne_chunk, word_tokenize, pos_tag\n",
        "import nltk\n",
        "\n",
        "def visualize_named_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "    tree = ne_chunk(tags)\n",
        "\n",
        "    tree.pretty_print()\n",
        "\n",
        "text = \"Apple Inc. is planning to acquire U.K.-based startup for $1 billion.\"\n",
        "visualize_named_entities(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnxQ-q28Brda",
        "outputId": "efb99e13-16fe-4a63-a809-086a3cf4e80e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                S                                                  \n",
            "   _____________________________________________________________|___________________________________________        \n",
            "  |         |         |       |            |           |        |     |   |       |       |    PERSON  ORGANIZATION\n",
            "  |         |         |       |            |           |        |     |   |       |       |      |          |       \n",
            "is/VBZ planning/VBG to/TO acquire/VB U.K.-based/JJ startup/NN for/IN $/$ 1/CD billion/CD ./. Apple/NNP   Inc./NNP  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQ_2j7sAC7GN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}